{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.97227906e+05   3.27695639e+05   3.66534376e+05   2.30449798e+05\n",
      "    5.36292439e+05   2.97896096e+05   3.12795868e+05   1.97669307e+05\n",
      "    2.10583436e+05   2.40879638e+05   2.38395349e+05   3.44681378e+05\n",
      "    3.27793977e+05   6.95223334e+05   2.58163372e+05   4.46893810e+05\n",
      "    2.97896096e+05   1.98564287e+05   4.96657060e+05   5.94997538e+05\n",
      "    2.51210146e+05   2.53296114e+05   2.41276965e+05   2.58163372e+05\n",
      "    5.70065254e+05   2.48230192e+05   4.61396254e+05   4.65866186e+05\n",
      "    4.71826094e+05   2.97896096e+05   3.47562001e+05   1.68764744e+05\n",
      "    3.12795868e+05   5.76025162e+05   2.83989643e+05   2.48230192e+05\n",
      "    2.28363830e+05   3.42694742e+05   5.45331633e+05   2.85082293e+05\n",
      "    3.66037717e+05   3.27695639e+05   3.11901881e+05   2.97002110e+05\n",
      "    1.78697925e+05   2.97896096e+05   2.37899683e+05]\n",
      " [ -1.25932067e-11  -1.08978416e-11  -1.18528674e-11  -7.73432061e-12\n",
      "   -1.92434349e-11  -1.03288399e-11  -1.03312585e-11  -6.22793756e-12\n",
      "   -6.74840411e-12  -8.14912864e-12  -7.95180571e-12  -1.15754639e-11\n",
      "   -1.09321158e-11  -2.33391494e-11  -8.75190417e-12  -1.49260913e-11\n",
      "   -1.03288399e-11  -6.25694094e-12  -1.69282715e-11  -2.06683716e-11\n",
      "   -8.56696656e-12  -8.62116938e-12  -8.06745347e-12  -8.75190417e-12\n",
      "   -2.06768077e-11  -8.46942678e-12  -1.54272053e-11  -1.55718071e-11\n",
      "   -1.59346618e-11  -1.03288399e-11  -1.16647133e-11  -5.67011111e-12\n",
      "   -1.03312585e-11  -2.06623189e-11  -1.03348459e-11  -8.46942678e-12\n",
      "   -7.59429060e-12  -1.15452945e-11  -2.06715405e-11  -1.03362085e-11\n",
      "   -1.18735452e-11  -1.08978416e-11  -1.03312088e-11  -1.03285907e-11\n",
      "   -5.89020220e-12  -1.03382135e-11  -8.02516433e-12]\n",
      " [  1.74529777e-11   1.72812546e-11   1.73525962e-11   8.87226116e-12\n",
      "    2.39359103e-11   1.69518072e-11   1.72646110e-11   8.71691541e-12\n",
      "    8.78664967e-12   8.92594265e-12   8.89854239e-12   1.72990185e-11\n",
      "    1.73087394e-11   3.45793012e-11   9.05851957e-12   1.76799066e-11\n",
      "    1.69518072e-11   8.73330804e-12   1.79905236e-11   3.39630355e-11\n",
      "    9.01169524e-12   9.02819729e-12   8.91903597e-12   9.05851957e-12\n",
      "    3.35284070e-11   8.99749892e-12   1.77310813e-11   1.77632378e-11\n",
      "    1.78157676e-11   1.69518072e-11   1.72854311e-11   8.63667952e-12\n",
      "    1.72646110e-11   3.36344444e-11   1.67339743e-11   8.99749892e-12\n",
      "    8.84794140e-12   1.72891225e-11   2.97181134e-11   1.67803391e-11\n",
      "    1.73473988e-11   1.72812546e-11   1.72516644e-11   1.69384609e-11\n",
      "    8.65956010e-12   1.69482451e-11   8.90473223e-12]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import loadtxt, mean, std, ones\n",
    "from pylab import show, scatter\n",
    "import pandas as pd\n",
    "\n",
    "def feature_normalize(X):\n",
    "    m = X.shape\n",
    "    X_norm = X\n",
    "    for i in range(m[1]):\n",
    "        mu = mean(X[:, i])\n",
    "        sigma = std(X[:, i])\n",
    "        X_norm[:, i] = (X[:, i] - mu) / sigma\n",
    "    \n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "# function to minimize square error cost\n",
    "def gradient_descent(X, Y, theta, alpha, iters):\n",
    "    m = X.size\n",
    "    for i in range(iters):\n",
    "        A = X.dot(theta) - Y\n",
    "        delta = 1.0 / m * (A.T.dot(X).T)\n",
    "        theta = theta - (alpha * delta)\n",
    "    return theta\n",
    "\n",
    "# load labled training set\n",
    "df = loadtxt('ex1data2.txt', delimiter=',')\n",
    "\n",
    "# Assign labeld training set to independent and depended variables\n",
    "X = df[:,:2]\n",
    "Y = df[:,2]\n",
    "\n",
    "# Normalize features to avoid ending in local optima\n",
    "X, mu, sigma = feature_normalize(X)\n",
    "\n",
    "# Count the number of training examples\n",
    "m = X.shape\n",
    "\n",
    "# X subscript 1 = 0\n",
    "it = ones(shape=(m[0], 3))\n",
    "it[:, 1:3] = X\n",
    "\n",
    "# initial theta value\n",
    "theta = zeros(shape=(3, 1))\n",
    "\n",
    "# initial alpha value\n",
    "alpha = 0.01\n",
    "\n",
    "# iterations count\n",
    "iters = 1500\n",
    "\n",
    "# gradient descent algorithm to minimize cost\n",
    "theta = gradient_descent(it, Y, theta, alpha, iters)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
